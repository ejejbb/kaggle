# 1. 캐글 필사: Porto Seguro

## ✅ 목적
> 예측 모델링에서 워크 플로우가 어떻게 진행되는지에 대한 감 잡기
- 피쳐 체크하기
- 새로운 피쳐 생성하기
- ML 개념

## 1. Introduction

**R 환경**에서 *tidyverse*와 *ggplot2*를 활용한 **Porto Seguro의 Safe Driver Prediction 대회**에 대한 포괄적인 탐색적 데이터 분석(EDA)이다.   
- 다양한 데이터 feature들을 **시각화**
- 각 feature들이 **타깃 변수**와 어떤 관련이 있는지 살펴보기
- 여러 변수 간의 **상호작용**을 탐색
- **feature engineering**을 수행

이 대회의 목표는 **운전자가 보험 청구를 할 가능성**을 예측하는 것으로, 개인의 운전 습관을 바탕으로 보다 공정한 보험료를 제공하는 것이 목적이다.

각각의 행(row)은 **특정 보험 가입자(policy holder)** 를 나타내며, 열(columns)은 이 가입자에 대한 다양한 피쳐 정보를 담고 있다. 예측 대상이 되는 변수는 **target**이라는 이름으로 주어지며, 해당 가입자가 과거에 보험금을 청구한 적이 있는지를 나타낸다.


## 2. Preparations
### 2.1 Load libraries
```
# general visualisation
# general data manipulation
# specific visualisation
# modelling
```
### 2.2 Helper functions
```
# multiplot 함수: 다중 패널 플롯을 생성
# 이항 신뢰 구간을 계산하기 위한 간단한 보조 함수를 사용
```
### 2.3 Load data
```
# 데이터 불러오기: data.table 패키지의 fread 함수를 사용하여 불러오기, 속도 향상이 목적
# 원본 데이터의 결측치가 -1 또는 -1.0으로 표시되어 있다는 점을 고려하여, 이를 적절한 NA 값으로 변환
```

## 3. Overview: File structure and content
### 3.1 Training data
```
# summary(train)
# glimpse(train) -- str()의 가독성 좋은 버전
```
### 3.2 Test data
```
# summary(test)
# glimpse(test)
```
### 3.3 Missing values
```
# sum(is.na(train))
# sum(is.na(test))

## 결측치의 수가 매우 많음
```
### 3.4 Reformating features
```
# 범주형 변수(categorical features) -> factor
# 이진 변수(binary features) -> logical
# 타겟 변수(target variable) -> factor

# factor: 고유한 level로 기억하여 그룹별 분석이나 모델링 시 자동 더미 변수 처리 가능
# logical: TRUE/FALSE로 나타남
```
### 3.5 Combining data frames
```
# train과 test에 대해 동일한 처리를 위해 결합
```
## 4. Individual feature visualisations
```
# 변수들의 분포 확인하기
- 변수들을 특정 그룹별로 나누어 레이아웃을 구성
- 가독성을 높이기 위해, 각 그룹을 여러 부분으로 나누어 시각화
- 분석을 시작할 때는 변수 공간(parameter space)에 대해 명확하게 파악하는 것이 중요
```
### 4.1 Binary features part 1
```
# ps_ind_06_bin ~ ps_ind_13_bin까지 총 8개의 이진 변수에 대해
# 각 변수의 막대 그래프(빈도수 분포)를 개별 플롯으로 만들고,
# 이들을 2행 4열 레이아웃으로 한 화면에 나란히 출력

## 값이 매우 불균형 (FALSE가 대부분을 차지)
```
![alt text](/kaggle_study/image/image1.png)

### 4.2 Binary features part 2
```
# ps_ind_16_bin ~ ps_ind_18_bin, ps_calc_15_bin ~ ps_calc_20_bin까지 총 9개의 이진 변수에 대해
# 각 변수의 막대 그래프(빈도수 분포)를 개별 플롯으로 만들고,
# 이들을 2행 5열 레이아웃으로 한 화면에 나란히 출력

## part1에서 확인했던 변수보다는 균형잡힌 분포
```
![alt text](/kaggle_study/image/image2.png)

### 4.3 Categorical features part 1
```
# _cat으로 끝나는 범주형 변수들의 값 분포(막대 그래프)를 시각화
# 값의 빈도 차이가 클 수 있으므로 Y축을 로그 스케일로 변환
# 6개의 플롯을 3행 2열로 정렬해서 한 화면에 보기
```
![alt text](/kaggle_study/image/image3.png)

### 4.4 Categorical features part 2
```
# ps_car_04_cat부터 ps_car_11_cat까지의 범주형(_car) 변수들의 분포를 로그 스케일로 시각화
# 총 8개의 플롯을 4행 4열로 배치

## ps_car_11 변수는 상대적으로 많은 수준(level)을 가진다.
```
![alt text](/kaggle_study/image/image4.png)

### 4.5 Integer features part 1: “ind” and “car”
```
# ind와 car 그룹에 속한 정수형 변수(integer features)들을 (bar) 플롯으로 시각화
# int 값을 갖고 있어도 이산적인 값을 가지는 경우, factor 형태로 변환하는 것이 시각화에 유리
```
![alt text](/kaggle_study/image/image5.png)

### 4.6 Integer features part 2: “calc”
```
# ps_calc_04 ~ ps_calc_14의 분포를 시각화
# 대부분은 막대그래프(barplot)로, 일부는 히스토그램(histogram)으로 표현
# 총 11개의 변수 시각화를 3행 4열 레이아웃으로 배치

## 10, 11, 14의 히스토그램을 보면, 정규 분포(normal distribution)에 가까운 형태를 보이며, 큰 값 쪽으로 꼬리가 두드러지게 퍼져 있는 경향도 확인된다.

## 그 외의 변수들도 정규 분포 혹은 로그 정규 분포(log-normal distribution)에서 크게 벗어나지 않는다.
```
![alt text](/kaggle_study/image/image6.png)

### 4.7 Float features part 1: “reg” and “calc”
![alt text](/kaggle_study/image/image7.png)
### 4.8 Float features part 2: “car”
![alt text](/kaggle_study/image/image8.png)
### 4.9 Target variable
![alt text](/kaggle_study/image/image9.png)
```
# A tibble: 2 × 2
  target percentage
  <fct>       <dbl>
1 0           96.4 
2 1            3.64
```
### 4.10 More details on missing values
![alt text](/kaggle_study/image/image10.png)
```R
> sum(is.na(train))/(nrow(train)*ncol(train))*100
[1] 2.410359
> 
> sum(is.na(test))/(nrow(test)*ncol(test))*100
[1] 2.453096
```

## 5. Claim rates for individual features
### 5.1 Binary features part 1
![alt text](/kaggle_study/image/image11.png)
### 5.2 Binary features part 2
![alt text](/kaggle_study/image/image12.png)
### 5.3 Categorical features part 1
![alt text](/kaggle_study/image/image13.png)
### 5.4 Categorical features part 2
![alt text](/kaggle_study/image/image14.png)
### 5.5 Integer features part 1
![alt text](/kaggle_study/image/image15.png)
### 5.6 Integer features part 2
![alt text](/kaggle_study/image/image16.png)
### 5.7 Float features part 1
![alt text](/kaggle_study/image/image17.png)
### 5.8 Float features part 2
![alt text](/kaggle_study/image/image18.png)
### 5.9 Summary
```
# 클레임 발생률(claim rate)은 변수 그룹 내에서도, 그룹 간에도 뚜렷한 차이를 보임.

# 일부 이진 변수(binary features)는 1~2%포인트 수준의 유의미한 효과를 보이는 반면,
# 다른 변수들은 실질적인 영향이 거의 없는 경우도 확인됨.  

# 특히, 대부분의 `ind` 그룹 변수들은 클레임 발생에 명확한 영향을 미치는 반면, `calc` 그룹의 이진 변수들은 중립적(=영향 없음)인 특성을 보임.
# `calc` 그룹 전체가 이번 예측 목적에 있어 즉각적인 유용성이 낮을 수 있다는 점을 시사

# 가장 큰 영향을 미친 것은 범주형(categorical) 및 정수형(integer) 변수
# 특히 `ind`와 `car` 그룹 변수들은 2~3%포인트에 이르는 차이를 만들어내는 경우도 있었음
# 반면, 실수형 변수(floating point features)는 영향이 미미하지만, 정밀한 예측 성능 향상을 위해서는 사용하는 것이 좋겠음.
```

## 6 Multi-feature comparisons
### 6.1 Correlation overview
![alt text](/kaggle_study/image/image19.png)
### 6.2 Alluvial diagram for key features
![alt text](/kaggle_study/image/image20.png)
### 6.3 Exploring correlated features
### 6.3.1 Pairwise relationships
![alt text](/kaggle_study/image/image21.png)
### 6.4 Interactive multi-dimensional relations
#### 6.4.1 Multi-parameter grid
![alt text](/kaggle_study/image/image22.png)
#### 6.4.2 The impact of uncorrelated high-signal features
![alt text](/kaggle_study/image/image23.png)

=> 6까지는 eda

## 7. Feature engineering
### 7.1 Number of NAs per ID
```
# 대부분의 ID는 NA가 2개 이하 (2개인 경우가 가장 빈도 높음)
# 그 이후부터는 NA 개수가 증가할수록 해당 사례 수는 급격히 줄어들며, NA가 5개를 초과하는 경우는 매우 드물다
# NA가 정확히 5개인 ID는 존재하지 않는데, 이는 아마도 특정 변수들 간의 구조적인 연관성을 반영하는 결과일 수 있다.
# 또한 NA가 7개인 지점에서 작은 두 번째 피크가 나타나는 것도 주목할 만하며, 이 역시 데이터 내 숨겨진 구조적인 패턴을 시사할 수 있다.
```
![alt text](/kaggle_study/image/image24.png)

### 7.2 Sum of binary features
```
[이진변수 합산]
# ind
- 최빈값은 2
- 값의 범위는 1에서 6 사이
- 0은 없음 -> 하나 이상의 이진 플래그가 항상 설정되어 있다(=TRUE)는 것을 의미
- ps_ind_??_bin 변수는 총 11개였지만, 동시에 "TRUE" 또는 1로 설정된 플래그는 최대 6개에 불과함
- 이는 대부분의 이진 변수가 "FALSE" 또는 0 값을 주로 가지는 경향과 일치함
- 합이 커질수록 클레임 발생률이 증가하는 경향
- 특히 "3"과 "1" 또는 "2" 사이의 차이는 통계적으로 유의미함 -> 이 특성은 향후 예측 모델에 활용 가능한 유용한 파생 변수가 될 수 있음

# calc
- 클레임 발생률에서 뚜렷한 차이를 보이지 않음 -> calc 변수는 예측에 큰 도움 되지 않을 수 있음
- 최빈값은 2, 값의 범위는 0부터 6까지
```
![alt text](/kaggle_study/image/image25.png)
### 7.3 Difference measure for binary features
```
[이진값의 기준값과의 차이]
# ind
- 주로 1과 3에서 많이 나타남
- 차이값이 0인 경우는 전혀 존재하지 않음 => 기준값과 일치하는 행은 존재하지 않음을 의미
- 클레임 발생률을 보면, 기준 행으로부터 멀어질수록 클레임 비율이 증가하는 경향이 나타남
- 하지만 통계적으로 유의미한 차이는 차이값 1, 2와 차이값 3, 4 사이에서만 확인되며, 나머지 차이값들 간의 차이는 통계적 유의성이 없음.

# calc
- 유의미x
```
![alt text](/kaggle_study/image/image26.png)

## 8. Modelling preparation
- 데이터가 제대로 준비되었는지 확인하기 위한 몇 가지 점검(sanity check)을 진행
- 모델 학습(model fitting)에 적합한 형태로 데이터를 정리

### 8.1 Train vs test comparison
```
# train과 test가 실제로 유사한 특성 공간(parameter space)을 공유하는지 확인하는 것이 중요하다.

-> train에서 특정 피처가 타겟 변수와 강한 상관관계를 가지고 있다 하더라도, 그 피처의 핵심적인 값들이 test에는 존재하지 않는다면 그 피처는 예측 모델에서 실질적인 도움이 되지 않게 되기 때문
```
```
# 1. ID 값의 분포
# 2. 이진 변수
- 각 수준(level)별로 train 데이터가 차지하는 비율을 시각화하여 비교
# 3. 범주형 변수
- 이진 변수와 동일하게 각 수준에 대해 train / (train + test) 비율을 계산하여 시각화
# 4. 실수형 변수
- train 데이터와 test 데이터를 겹쳐서 비교
```
![alt text](/kaggle_study/image/image27.png)
![alt text](/kaggle_study/image/image28.png)
![alt text](/kaggle_study/image/image29.png)
```
[결과 해석]
# ID 값의 분포
- 무작위 분포

# 이진 변수
- 각 수준(level)에서 train / (train + test) 비율은 매우 유사하게 나타남
- 평균적으로 각 변수의 "TRUE"와 "FALSE" 수준에 대해
train 데이터가 약 40% 정도를 차지
- 비율에서 큰 차이를 보이는 수준(level)들은 대부분 샘플 수가 적어 통계적 오차가 큼

# 범주형 변수
- 대부분의 수준에서 train 데이터의 비율이 약 40%로 매우 일관되게 나타남

# 실수형 변수
- train과 test 데이터 간의 분포가 거의 완벽하게 겹쳐짐
```
### 8.2 Feature selection, evaluation metric, and validation split
### 8.3 XGBoost parameters and fitting
### 8.4 Feature importance
### 8.5 Prediction and submission file