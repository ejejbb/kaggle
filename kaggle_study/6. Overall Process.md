# 1. 캐글 필사
> 수치 예측 기반 회귀를 중심으로

```
[모델링 전반 프로세스]
데이터 로딩
    ↓
EDA (결측, 분포, 이상치, 상관성, 시각화)
    ↓
Feature Engineering (파생변수 생성, 결측 처리, 클러스터링 등)
    ↓
데이터 정제 및 스케일링
    ↓
모델 학습 (CV, 튜닝, 로그변환 고려)
    ↓
평가(RMSE, MAE 등)
    ↓
예측 결과 제출
```
---
## 🔍 1. EDA

**목표: 데이터를 이해하고, 품질 문제를 확인해 피처 설계 방향 결정하기**

### 📌 주요 체크포인트

| 항목        | 설명                                                               |
| --------- | ---------------------------------------------------------------- |
| 결측치 확인    | `isnull().sum()` or `df.describe()`<br>→ 특정 변수에 결측이 많다면 제거/대체 고려 |
| 타깃 변수 분포  | 히스토그램 + 로그 변환 확인<br>`plt.hist(y)` → 비대칭이면 `np.log1p(y)` 고려       |
| 변수 간 상관관계 | `df.corr()` or `sns.heatmap`<br>→ 타깃과 상관 높은 변수부터 우선순위            |
| 변수별 이상치   | `boxplot`, `scatterplot` 등으로 시각화                                 |
| 시계열 여부    | 시간에 따른 패턴이 있다면 시간 파생변수 생성 고려                                     |

### 결측치 대체

결측치 처리는 데이터 전처리 중 가장 중요한 작업 중 하나이다. 결측치를 잘못 다루면 모델 성능이 떨어지거나, 학습 자체가 안 될 수 있다는 문제가 발생할 수 있다. 그러나 반드시 모든 상황에서 결측치를 대체해야 하는 것은 아니다.

결측치를 처리하지 않고 그대로 둬도 되는 경우는 다음과 같다.   
① 결측 그 자체가 의미 있는 정보인 경우   
② Tree 모델을 사용하는 경우   
: `XGBoost`, `LightGBM`, `CatBoost`는 결측치가 있어도 학습 가능하지만, `sklearn.RandomForestRegressor`는 결측치가 있으면 모델 학습이 불가능하다.

결측치를 반드시 채워야 하는 경우는 다음과 같다.   
① 선형 모델을 사용하는 경우   
② 거리 기반 모델을 사용하는 경우   
③ 신경망 모델을 사용하는 경우   
④ 평가지표 계산 시    

### 결측치 대체 방법

① 단순 대체법: 평균, 중앙값, 최빈값, 고정값(예시:0)으로 대체   
② 조건부 대체법: 그룹별 평균/중앙값 등 대체, 시간 기반 보간(시계열 특성 반영) 등   
③ 보간법: 선형보간, 다항식 보간 등   
④ 모델 기반 보간: `KNN Imputer`(결측치가 있는 값과 비슷한 값을 가진 것의 값으로 대체), `Iterative Imputer`(다른 변수로 회귀 모델 학습 후 결측값 예측)    

```python
# iterative imputer 예시
imputer = IterativeImputer(estimator=BayesianRidge(), max_iter=10, random_state=42)
## estimator -> LinearRegression(), Ridge(), DecisionTreeRegressor(), RandomForestRegressor(), KNeighborsRegressor() 등으로 대체 가능

X_imputed = imputer.fit_transform(X)
```
---

## 🛠️ 2. Feature Engineering
> 기존 피쳐를 변형하여 타깃 변수 예측을 더 용이하게 하기

### 1. 범주형 변수 처리
| 방법                 | 설명                     | 예시                      |
| ------------------ | ---------------------- | ----------------------- |
| Label Encoding     | 문자 → 숫자 레이블 (순서 있음)    | `low=0, mid=1, high=2`  |
| One-Hot Encoding   | 각 범주 → 이진 피처           | `pd.get_dummies()`      |
| Frequency Encoding | 값이 등장한 횟수로 치환          | `'A' → 100, 'B' → 20`   |
| Target Encoding    | 범주별 평균 타깃값으로 치환        | `'A' → 평균 발전량 250`      |
| Embedding          | 딥러닝용, dense vector로 변환 | `PyTorch, Keras` 등에서 사용 |

### 2. 수치형 변수 처리
| 기법                    | 설명                               | 목적                            |
| --------------------- | -------------------------------- | ----------------------------- |
| 로그 변환                 | `np.log1p(x)`                    | 오른쪽 꼬리가 긴 분포 정규화              |
| Box-Cox / Yeo-Johnson | 다양한 분포 정규화 기법                    | `power_transform()`           |
| 스케일링                  | `StandardScaler`, `MinMaxScaler` | 거리 기반 모델(SVM, KNN, NN 등)에서 중요 |

### 3. 파생 변수 생성
| 방법            | 설명                              | 예시                  |
| ------------- | ------------------------------- | ------------------- |
| 날짜/시간 분해      | `datetime → day, hour, weekday` | 풍속 시계열에서 시간대별 성능 차이 |
| 구간화 (Binning) | `pd.cut`, `pd.qcut`             | 풍속을 구간별로 나누기        |
| 조합 피처         | `A * B`, `A / B` 등              | 발전량 = 풍속 \* 밀도      |
| 텍스트 길이, 단어 수  | 텍스트의 구조적 특징 수치화                 | 리뷰 길이, 문장 수         |
| 위치 정보 → 거리 계산 | 위도/경도 → Haversine 거리            | 발전소 간 거리 등          |

* 시계열 데이터에서는 lag변수나 diff변수를 잘 이용하면 좋은 결과를 얻을 수 있음.
* 모델을 한 번 돌려보고, feature importance를 시각화해서 불필요한 피처 제거하는 것도 좋은 전략이 될 수 있음.

---

## 🤖 3. Modeling

### 📌 회귀 문제 기준 주요 모델

| 모델                     | 특징                | 장점                 |
| ---------------------- | ----------------- | ------------------ |
| Linear / Ridge / Lasso | 선형 기반             | 빠르고 해석 용이          |
| Random Forest          | 앙상블 트리            | 비선형 대응, 이상치에 강함    |
| XGBoost / LightGBM     | Gradient Boosting | 예측력 뛰어나고 커스텀 손실 가능 |
| CatBoost               | 범주형 처리 특화         | 인코딩 없이도 성능 우수      |
| Neural Network         | 복잡한 구조 대응         | 대규모 데이터에 유리        |

### 📌 회귀 모델 튜닝 시 고려할 것

* Cross-validation: K-Fold (Stratified는 분포 다를 때만)
* 로그 변환 시 예측 후 `np.expm1()` 다시 복원
* 과적합 방지: early stopping, regularization 사용

---

## 📏 4. 평가 및 최종 제출

### 📌 NMAE 기준 평가 지표

* 작은 값도 오차가 크게 반영됨
* 예측값이 음수가 되지 않도록 후처리 필요 (`np.clip(pred, 0, None)`)